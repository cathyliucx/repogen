"""
Dependency-Ordered Dataset Generator (Config-Driven with Breakpoint Resumption)
"""

import os
import sys
import ast
import json
import argparse
import logging
import random
import yaml
from pathlib import Path
from typing import Dict, List, Optional, Any, Set
from collections import defaultdict
import tiktoken 

# Setup Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("dataset_generator")
# Suppress INFO logs from httpx (hide POST/GET request details)
logging.getLogger("httpx").setLevel(logging.WARNING)
# Suppress logs from the underlying HTTP engine
logging.getLogger("httpcore").setLevel(logging.WARNING)

from dependency_analyzer import (
    CodeComponent, 
    DependencyParser, 
    dependency_first_dfs, 
    build_graph_from_components
)
from visualizer import ProgressVisualizer
from agent.task.orchestrator import Orchestrator

def load_yaml(path: str) -> dict:
    """Load and parse the configuration settings from a YAML file.

    Args:
        path: The file path to the YAML configuration.

    Returns:
        A dictionary containing the parsed YAML content.
    """
    if not os.path.exists(path):
        logger.error(f"Config file not found: {path}")
        sys.exit(1)
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f) or {}

def get_processed_components(output_path: str) -> Set[str]:
    """Scan the existing output file to identify components already processed in previous runs.

    Args:
        output_path: Path to the JSONL output file.

    Returns:
        A set of component IDs that have already been saved to the output.
    """
    processed = set()
    if os.path.exists(output_path):
        try:
            with open(output_path, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        data = json.loads(line)
                        if "component_id" in data:
                            processed.add(data["component_id"])
                    except json.JSONDecodeError:
                        continue
            
            if processed:
                logger.info(f"Audit: Found {len(processed)} existing records in the output file.")
        except Exception as e:
            logger.warning(f"Could not read existing output for resumption: {e}")
    return processed

def generate_datasets_for_component(
    component: CodeComponent, 
    orchestrator: Optional[Orchestrator], 
    test_mode: str = 'none',
    dependency_graph: Optional[Dict[str, List[str]]] = None
) -> Any:
    """Generate the QA or Design dataset content for a specific code component via the LLM Orchestrator.

    Args:
        component: The code component object containing source and metadata
        orchestrator: The Orchestrator instance managing the AI agents
        test_mode: Testing flags to skip actual LLM calls if configured
        dependency_graph: Map of dependencies used for context retrieval

    Returns:
        Dataset content generated by the LLM agents or None if processing fails.
    """
    if not orchestrator:
        return None
    
    file_path = component.file_path
    component_code = component.source_code
    file_path_for_orchestrator = getattr(component, "relative_path", None) or file_path
    
    encoding = tiktoken.get_encoding("cl100k_base")
    token_consume_focal = len(encoding.encode(component_code))
    
    # Simple truncation for extremely large components
    if token_consume_focal > 10000:
        component_code = encoding.decode(encoding.encode(component_code)[:10000])
    
    with open(file_path, "r", encoding="utf-8") as f:
        file_content = f.read()
    
    ast_tree = ast.parse(file_content)
    ast_node = None
    component_parts = component.id.split(".")
    component_name = component_parts[-1]
    
    # Locate the AST node for context injection
    if component.component_type == "function":
        for node in ast.iter_child_nodes(ast_tree):
            if (isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)) 
                    and node.name == component_name):
                ast_node = node
                break
    elif component.component_type == "class":
        for node in ast.iter_child_nodes(ast_tree):
            if isinstance(node, ast.ClassDef) and node.name == component_name:
                ast_node = node
                break
    elif component.component_type == "method":
        if len(component_parts) >= 2:
            class_name, method_name = component_parts[-2:]
            for node in ast.iter_child_nodes(ast_tree):
                if isinstance(node, ast.ClassDef) and node.name == class_name:
                    for item in node.body:
                        if (isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)) 
                                and item.name == method_name):
                            ast_node = item
                            break
                    break
    
    try:
        datasets = orchestrator.process(
            focal_component=component_code,
            file_path=file_path_for_orchestrator,
            ast_node=ast_node,
            ast_tree=ast_tree,
            dependency_graph=dependency_graph,
            focal_node_dependency_path=component.id,
            focal_component_type=component.component_type,
            token_consume_focal=token_consume_focal
        )
        return datasets
    except Exception as e:
        logger.error(f"Error generating data for component {component.id}: {str(e)}")
        return None

def main():
    """Execute the dataset generation workflow with dependency ordering and breakpoint support.

    Args:
        No direct arguments; parses CLI parameters via argparse.

    Returns:
        None; outputs generated results to a JSONL file and logs process to a file.
    """
    parser = argparse.ArgumentParser(description='Generate datasets in dependency order.')
    parser.add_argument('--agent-config', type=str, default='config/agent_config.yaml', help='Path to agent configuration')
    parser.add_argument('--data-config', type=str, default='config/data_config.yaml', help='Path to data configuration')
    parser.add_argument('--repo-dir', type=str, help='Override repository directory')
    parser.add_argument('--output', type=str, help='Force override output path')
    
    args = parser.parse_args()

    agent_cfg = load_yaml(args.agent_config)
    data_cfg = load_yaml(args.data_config)

    task_type = agent_cfg.get('task', 'design').lower()
    settings = agent_cfg.get('settings', {})
    test_mode = settings.get('test_mode', 'none')
    order_mode = settings.get('order_mode', 'topo')

    repo_path = args.repo_dir or data_cfg.get('repo_dir', 'data/raw_test_repo')
    rag_path = data_cfg.get('rag_dir', 'data/meta_test_repo/repo_wiki/WIKI.md')
    components_json = data_cfg.get('repo_dependency_dir', 'data/meta_test_repo/dependency_graph.json')
    
    if args.output:
        output_path_str = args.output
    else:
        output_key = 'design_out_dir' if task_type == 'design' else 'qa_out_dir'
        output_path_str = data_cfg.get(output_key, f'output/{task_type}_results.jsonl')

    output_path = Path(output_path_str)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    # NON-DESTRUCTIVE LOGGING: Use mode='a' to append to history
    log_file_path = output_path.parent / f"run_{task_type}.log"
    file_handler = logging.FileHandler(log_file_path, mode='a', encoding="utf-8")
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
    logging.getLogger().addHandler(file_handler)
    
    # VISUAL SEPARATOR FOR NEW SESSIONS
    logger.info("\n" + "="*80)
    logger.info(f"STARTING NEW EXECUTION SESSION: {task_type.upper()}")
    logger.info(f"Target Repo: {repo_path} | Output: {output_path_str}")
    logger.info("="*80)
    
    # BREAKPOINT: Identify already processed components
    processed_cids = get_processed_components(output_path_str)
    
    orchestrator_test_mode = test_mode if test_mode != 'none' else None
    orchestrator = Orchestrator(
        repo_path=repo_path,
        config_path=args.agent_config,
        test_mode=orchestrator_test_mode,
        rag_path=rag_path,
        run_log_path=str(log_file_path),
    )
    
    components: Dict[str, CodeComponent] = {}
    dependency_graph: Dict[str, List[str]] = {}

    # Load from JSON or Parse from Scratch
    if components_json and os.path.exists(components_json):
        logger.info(f"Loading components from JSON: {components_json}")
        with open(components_json, "r", encoding="utf-8") as f:
            raw = json.load(f)

        for cid, data in raw.items():
            comp = CodeComponent.from_dict(data)
            try:
                if comp.file_path and os.path.exists(comp.file_path) and comp.start_line and comp.end_line:
                    with open(comp.file_path, "r", encoding="utf-8") as rf:
                        lines = rf.read().splitlines()
                    comp.source_code = "\n".join(lines[int(comp.start_line) - 1 : int(comp.end_line)])
            except Exception as e:
                logger.warning(f"Could not load source_code for {cid}: {e}")
            components[cid] = comp
            dependency_graph[cid] = list(comp.depends_on) if hasattr(comp, "depends_on") else []
        graph = build_graph_from_components(components)
    else:
        logger.info(f"Parsing repository: {repo_path}")
        parser_obj = DependencyParser(repo_path)
        components = parser_obj.parse_repository()
        graph = build_graph_from_components(components)
        dependency_graph = {cid: list(deps) for cid, deps in graph.items()}
    
    # Topological Sorting
    sorted_components = dependency_first_dfs(graph)
    
    # Resumption Logic: Log remaining workload
    remaining_tasks = [cid for cid in sorted_components if cid not in processed_cids]
    if processed_cids:
        logger.info(f">>> [RESUME MODE] Already finished: {len(processed_cids)}")
        logger.info(f">>> [RESUME MODE] Remaining tasks: {len(remaining_tasks)} / {len(sorted_components)}")
        if remaining_tasks:
            logger.info(f">>> [RESUME MODE] Next in queue: {remaining_tasks[0]}")
    else:
        logger.info(">>> [FRESH START] No previous records found.")

    visualizer = ProgressVisualizer(components, sorted_components)
    visualizer.initialize()
    
    # SYNC PROGRESS BAR: Immediately color existing results green
    for cid in sorted_components:
        if cid in processed_cids:
            visualizer.update(cid, "completed")
    
    # MAIN GENERATION LOOP
    for component_id in sorted_components:
        # SKIP LOGIC
        if component_id in processed_cids:
            continue

        component = components.get(component_id)
        # Skip internal python constructors or missing components
        if not component or (component.component_type == "method" and component_id.endswith(".__init__")):
            visualizer.update(component_id, "completed")
            continue
        
        visualizer.update(component_id, "processing")
        logger.info(f"Processing component: {component_id}")

        dataset_item = generate_datasets_for_component(component, orchestrator, test_mode, dependency_graph)
        
        if dataset_item:
            # APPEND MODE 'a' for data persistence
            with open(output_path_str, "a", encoding="utf-8") as f:
                result_record = {
                    "component_id": component_id,
                    "task": task_type,
                    "file_path": component.file_path,
                    "dataset_content": dataset_item
                }
                f.write(json.dumps(result_record, ensure_ascii=False) + "\n")
            visualizer.update(component_id, "completed")
            logger.info(f"Saved: {component_id}")
        else:
            visualizer.update(component_id, "error")
            logger.error(f"Failed to generate output for: {component_id}")
    
    visualizer.finalize()
    logger.info(f"Session finished. Log preserved in: {log_file_path}")

if __name__ == "__main__":
    main()